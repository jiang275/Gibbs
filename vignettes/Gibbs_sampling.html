<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Meilin Jiang" />


<title>Gibbs: Gibbs Sampling for Simple Linear Regression</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Gibbs: Gibbs Sampling for Simple Linear Regression</h1>
<h4 class="author">Meilin Jiang</h4>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(Gibbs)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="co">#&gt; Loading required package: MASS</span></a></code></pre></div>
<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>In Bayesian Statistics, obtaining posterior distributions of parameters of interests is a primary objective. However, analytical solutions to posterior distributions do not exist all the time due to required complex integration. Sampling techniques become an important method to allow us to study posterior distributions numerically. A common alternative choice is numerical approximation of integration, which provides accurate estimations, but computation issues arise as the dimension of parameters increases. For higher dimensional posterior distributions, the Method of Composition is an option when the joint distribution could be factorized into a product of a univariate distribution and a series of corresponding conditional distributions, which is not feasible in most practical cases. As a well-known Markov chain Monte Carlo (MCMC) procedure, Gibbs sampling provides a way to tackle high-dimensional distribution by sampling from full conditional distribution of each parameter.</p>
<p>At the meantime, linear regression models play a crucial role in statistical modeling for its easiness of modeling, interpretation, and implementation. The R package “Gibbs” built was aim to offer a method of implementing Gibbs sampling of regression parameters, <span class="math inline">\(\beta_0,\beta_1, and\ \sigma^2\)</span> from a perspective of Bayesian analysis.</p>
</div>
<div id="method" class="section level2">
<h2>Method</h2>
<p>Gibbs sampling iteratively generates parameters from their full conditional distributions. A full conditional distribution of x is the probability distribution of x given all other parameters of interests. For d-dimensional parameters of interest, the d parameters could be sampled in different order. In the package “Gibbs”, four common Gibbs samplers are built to satisfy different needs of users.</p>
<ol style="list-style-type: decimal">
<li>Deterministic- or systematic scan Gibbs sampler:</li>
</ol>
<p>The d dimensions are visited in a fixed order.</p>
<p>We pick a starting position <span class="math inline">\(\boldsymbol\theta^0=(\theta^0_1,\theta^0_2,...,\theta^0_d)^T\)</span> and sample each parameter from its corresponding full conditional distribution repeatedly given the last available values.<br />
Here is the d steps of sampling at iteration (k+1):</p>
<ul>
<li><p>Step 1. Sample <span class="math inline">\(\theta_1^{(k+1)}\)</span> from <span class="math inline">\(p(\theta_1|\theta^k_2,...,\theta^k_{(d-1)},\theta^k_d,\boldsymbol{y})\)</span></p></li>
<li>Step 2. Sample <span class="math inline">\(\theta_2^{(k+1)}\)</span> from <span class="math inline">\(p(\theta_2|\theta^{(k+1)}_1,\theta^k_{3},...,\theta^k_d,\boldsymbol{y})\)</span><br />
.<br />
.<br />
.<br />
</li>
<li><p>Step d. Sample <span class="math inline">\(\theta_d^{(k+1)}\)</span> from <span class="math inline">\(p(\theta_d|\theta^{(k+1)}_1,...,\theta^{(k+1)}_{(d-1)},\boldsymbol{y})\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Random-scan Gibbs sampler:<br />
The d dimensions are visited in a random order.<br />
At each step, we randomly select a parameter from all parameters of interest <span class="math inline">\(\boldsymbol\theta=(\theta_1,\theta_2,...,\theta_d)^T\)</span> and sample a new value from its full conditional distributions. The step was repeated until we reach a desired number of iterations.</li>
</ol></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><p>Reversible Gibbs sampler:<br />
The d dimensions are visited in a particular order and then the order is reversed.<br />
For each step, we sample <span class="math inline">\((\theta_1,\theta_2,...,\theta_d)\)</span> individually from 1 to d, and then we sample them again but form d to 1.</p></li>
<li><p>Block Gibbs sampler:<br />
The d dimensions are split up into m blocks of parameters with respective sizes <span class="math inline">\(d_1,d_2,...,d_m\)</span> and corresponding parameter vectors <span class="math inline">\(\boldsymbol{\theta}_1,\boldsymbol{\theta}_2,...\boldsymbol{\theta}_m\)</span>. Then the distributions <span class="math inline">\(p(\boldsymbol{\theta}_k|\boldsymbol{\theta}_1,...,\boldsymbol{\theta}_{k-1},\boldsymbol{\theta}_{k+1},...,\boldsymbol{\theta}_m)\)</span>,<span class="math inline">\((k=1,...,m)\)</span> are sampled in a (most often) fixed order.</p></li>
</ol>
<p>To connect Gibbs sampling to Bayesian linear regression models, consider the simple linear model below: <span class="math display">\[\boldsymbol{y}=\beta_0 + \beta_1\boldsymbol{x}+\boldsymbol{\epsilon}\]</span> where <span class="math inline">\(\boldsymbol{x}=(x_1,...,x_n)^T\)</span>, <span class="math inline">\(\boldsymbol{y}=(y_1,...,y_n)^T\)</span> and <span class="math inline">\(\boldsymbol{\epsilon}=(\epsilon_1,...,\epsilon_n)^T\)</span>. We assume <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>.</p>
<p>In linear regression case, it is relatively more difficult to provide a subjective prior on regression parameters comparing to an informative prior on mean or variance. A non-informative prior should be used unless there is a strong evidence on other priors. Here, a popular choice of non-informative prior is <span class="math inline">\(p(\boldsymbol{\beta},\sigma^2) \propto \sigma^-2\)</span></p>
<p>To apply Gibbs sampling procedure, the full conditional distribution for each parameter is needed. Given the non-informative prior above, the full conditionals are derived as following:</p>
<p><span class="math display">\[p(\sigma^2|\beta_0,\beta_1,\boldsymbol{y})=Inv-\chi^2(n,s^2_{\boldsymbol{\beta}})\]</span> <span class="math display">\[p(\beta_0|\sigma^2,\beta_1,\boldsymbol{y})=N(r_{\beta_{1}} ,\sigma^2/n)\]</span> <span class="math display">\[p(\beta_1|\sigma^2,\beta_0,\boldsymbol{y})=N(r_{\beta_{0}} ,\sigma^2/\boldsymbol{x}^T\boldsymbol{x})\]</span> where <span class="math display">\[s^2_{\boldsymbol{\beta}}=\frac{1}{n}\sum(y_i-\beta_{0}-\beta_{1}x_i)^2\]</span> <span class="math display">\[r_{\beta_{1}}=\frac{1}{n}\sum(y_i-\beta_{1}x_i)\]</span> <span class="math display">\[r_{\beta_{0}}=\sum(y_i-\beta_{0})x_i/\boldsymbol{x}^T\boldsymbol{x}\]</span></p>
<p>Also, <span class="math inline">\((\beta_0,\beta_1)\)</span> is often correlated and could be sampled together (block Gibbs sampler). The derived multivariate normal distribution and steps of sampling are listed as the following:</p>
<ul>
<li><p>Step 1: Sample <span class="math inline">\(\sigma2\)</span> from the full conditional specified above</p></li>
<li><p>Step 2: Sample vector(<span class="math inline">\(\beta_1, \beta_1\)</span>) from <span class="math display">\[p(\beta_0,\beta_1|\sigma^2,\boldsymbol{y})=N_{d=2}(\boldsymbol{\hat\beta},\sigma^2/\boldsymbol{x}^T\boldsymbol{x})\]</span> where <span class="math inline">\(\boldsymbol{\hat\beta}\)</span> is the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span></p>
<p>The package “Gibbs” consists of 8 functions that are nested within each other in order to implement different Gibbs sampler methods. Firstly, we can install the package from Github.</p></li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co"># require(devtools)</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="co"># install_github(&quot;https://github.com/jiang275/Gibbs&quot;)</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co"># library(Gibbs)</span></a></code></pre></div>
<p>The main function <em>Gibbs</em> is the actual function that will be applied by users. Users need to supply arguments of covariate vector x, response vector y, a method of Gibbs sampling (deterministic, random, reversible, or block). It is optional to input initial values of <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\beta_0\)</span>, and <span class="math inline">\(\beta_1\)</span>. The default number of iterations B is set to be 1000, and random seed generator is 123. Based on the input arguments, a matrix “draws” of missing values NA is created with B rows and 3 columns corresponding to <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\beta_0\)</span>, and <span class="math inline">\(\beta_1\)</span>. If users provided initial values for these parameters, the first row of matrix “draws” are replaced by initial values. If not, a linear model is fitted and the maximum likelihood estimators are used as initial values. Next, one of the second level functions (deterministic, random, reversible, or block) will be called based on the choice of Gibbs sampling methods. The prepared matrix “draws”, x, y, B, fit (fitted linearly model), and calculated n (number of observations) will be passed as arguments to the chosen second level function. The output from the second level function is the filled matrix “draws” with sampled values, which will be the final return element of the <em>Gibbs</em> function.</p>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<p>A simulated dataset is included in the package. Two vectors, x and y, are included in the “simulated_data” data frame. It is generated from the linear model <span class="math display">\[y_i = 2 + 0.7x_i +\epsilon_i \]</span> where <span class="math inline">\(\epsilon_i \sim N(0,1)\)</span></p>
<p>Here is the data generation process.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>,<span class="dv">8</span>,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">y &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.7</span><span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>,<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">simulated_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x,y)</a></code></pre></div>
<p>To access the dataset, simply type:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">data</span>(simulated_data)</a></code></pre></div>
<p>Knowing if true underlying values <span class="math inline">\(\beta_0=2,\beta_1=0.7,\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>, let us compare the performance of different Gibbs sampling on regression parameters.</p>
<p>The output is a matrix with B=1000 iterations of three regression parameters *<span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\beta_0\)</span>, and <span class="math inline">\(\beta_1\)</span>. The initial values of each parameters was chosen as 1. The posterior mean of each parameter is computed and compared with the truth.</p>
<ol style="list-style-type: decimal">
<li>Deterministic- or systematic scan Gibbs sampler:</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">draws &lt;-<span class="st"> </span><span class="kw">Gibbs</span>(<span class="dt">x=</span>simulated_data<span class="op">$</span>x,<span class="dt">y=</span>simulated_data<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">             <span class="dt">sigma2_initial=</span><span class="dv">1</span>, <span class="dt">beta0_initial=</span><span class="dv">1</span>,<span class="dt">beta1_initial=</span><span class="dv">1</span>,<span class="dt">method=</span><span class="st">&quot;deterministic&quot;</span>,<span class="dt">B=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb5-3" data-line-number="3"><span class="co"># posterior mean</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4"><span class="kw">apply</span>(draws, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb5-5" data-line-number="5"><span class="co">#&gt;    sigma2     beta0     beta1 </span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6"><span class="co">#&gt; 1.0211124 1.8505922 0.7236846</span></a></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Random-scan Gibbs sampler:</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">draws &lt;-<span class="st"> </span><span class="kw">Gibbs</span>(<span class="dt">x=</span>simulated_data<span class="op">$</span>x,<span class="dt">y=</span>simulated_data<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">             <span class="dt">sigma2_initial=</span><span class="dv">1</span>, <span class="dt">beta0_initial=</span><span class="dv">1</span>,<span class="dt">beta1_initial=</span><span class="dv">1</span>,<span class="dt">method=</span><span class="st">&quot;random&quot;</span>,<span class="dt">B=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="co"># posterior mean</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="kw">apply</span>(draws, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5"><span class="co">#&gt;    sigma2     beta0     beta1 </span></a>
<a class="sourceLine" id="cb6-6" data-line-number="6"><span class="co">#&gt; 1.0146938 1.8528077 0.7234478</span></a></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Reversible Gibbs sampler:</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">draws &lt;-<span class="st"> </span><span class="kw">Gibbs</span>(<span class="dt">x=</span>simulated_data<span class="op">$</span>x,<span class="dt">y=</span>simulated_data<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">             <span class="dt">sigma2_initial=</span><span class="dv">1</span>, <span class="dt">beta0_initial=</span><span class="dv">1</span>,<span class="dt">beta1_initial=</span><span class="dv">1</span>,<span class="dt">method=</span><span class="st">&quot;reversible&quot;</span>,<span class="dt">B=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="co"># posterior mean</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="kw">apply</span>(draws, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb7-5" data-line-number="5"><span class="co">#&gt;    sigma2     beta0     beta1 </span></a>
<a class="sourceLine" id="cb7-6" data-line-number="6"><span class="co">#&gt; 1.0240187 1.8359786 0.7252983</span></a></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Block Gibbs sampler:</li>
</ol>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">draws &lt;-<span class="st"> </span><span class="kw">Gibbs</span>(<span class="dt">x=</span>simulated_data<span class="op">$</span>x,<span class="dt">y=</span>simulated_data<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">             <span class="dt">sigma2_initial=</span><span class="dv">1</span>, <span class="dt">beta0_initial=</span><span class="dv">1</span>,<span class="dt">beta1_initial=</span><span class="dv">1</span>,<span class="dt">method=</span><span class="st">&quot;block&quot;</span>,<span class="dt">B=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb8-3" data-line-number="3"><span class="co"># posterior mean</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="kw">apply</span>(draws, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="co">#&gt;    sigma2     beta0     beta1 </span></a>
<a class="sourceLine" id="cb8-6" data-line-number="6"><span class="co">#&gt; 1.0188805 1.8641438 0.7222287</span></a></code></pre></div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>The package Gibbs provides a feasible way to sample parameters <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\beta_0\)</span>, and <span class="math inline">\(\beta_1\)</span> form simple linear regression models. Four types of Gibbs sampling methods (deterministic, random, reversible, or block) are available for use and comparison. In the testing case, different Gibbs sampler achieve comparably accurate estimators with a slightly better performance of block Gibbs sampler. For regression coefficients that are often correlated, it is beneficial to sample them simultaneously to account for the covariance. More iterations (B) could be generated if higher accuracy is desired, but the cost will also bee higher. It is also recommended to discard some iterations at beginning as the burn-in period since latter part of the chin is more stable for its convergence.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>[1] Lesaffre, E. &amp;Lawson, A. (2012). Bayesian Biostatistics. John Wiley &amp; Sons, Ltd. Retrieved from <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781119942412" class="uri">https://onlinelibrary.wiley.com/doi/book/10.1002/9781119942412</a></p>
<p>[2] Wakefield, J. (2013). Bayesian and Frequentist Regression Methods. Springer Series in Statistics. Retrieved from <a href="https://link.springer.com/book/10.1007/978-1-4419-0925-1" class="uri">https://link.springer.com/book/10.1007/978-1-4419-0925-1</a></p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
